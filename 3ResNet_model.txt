# 另一个非常经典的模型就是ResNet，这是一个残差模型，
# 核心思想是引入残差连接（shortcut connection）来缓解深度神经网络在训练过程中出现的梯度消失或梯度爆炸的问题，
# 它通过神经网络层提取的特征还要加上它的输入，来让它的特征与原始输入产生较强的联系，
# 所以这一模型最需要注意的就是提取特征后的特征图的通道数要与输入的通道数相等，以确保两者能够进行相加（类似线代的矩阵相加），
# 代码最后有构建如ResNet18等具体网络模块，可以自己根据参数带进去理解

"""
    Resnet (w. Quantized Operations)
"""
# torch
import torch
import torch.nn as nn
import torch.nn.functional as F


# custom
from utils.qutils import QuantizedConv2d,QuantizedLinear


# ----------------------------------------------------------------------------
#   ResNet
# ----------------------------------------------------------------------------
class BasicBlock(nn.Module):  # 残差网络架构中最基础的构建模块，核心思想是引入残差连接（shortcut connection）来缓解深度神经网络在训练过程中出现的梯度消失或梯度爆炸的问题
    expansion = 1  # 用于表示输出通道数相对于输入通道数的扩展倍数

    def __init__(self,in_planes,planes,stride=1):
        super(BasicBlock,self).__init__()
        self.conv1 = QuantizedConv2d(in_planes,planes,
                                     kernel_size=3,stride=stride,padding=1,bias=False) # 卷积
        self.bn1 = nn.BatchNorm2d(planes) # 对卷积输出进行批量归一化操作
        self.conv2 = QuantizedConv2d(planes,planes,
                                     kernel_size=3,stride=1,padding=1,bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()

        if stride != 1 or in_planes != self.expansion * planes:
            # 对输入特征图进行调整，使其能够和经过两层卷积后的特征图进行相加
            self.shortcut = nn.Sequential(
                QuantizedConv2d(in_planes,self.expansion * planes,
                        kernel_size=1,stride=stride,bias=False),
                nn.BatchNorm2d(self.expansion * planes)
            )

    def forward(self,x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out



class Bottleneck(nn.Module):  # 相较于BasicBlock更适合构建更深层次的网络，同时在保证性能的同时减少计算量
    expansion = 4

    def __init__(self,in_planes,planes,stride=1):
        super(Bottleneck,self).__init__()
        self.conv1 = QuantizedConv2d(in_planes,planes,kernel_size=1,bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = QuantizedConv2d(planes,planes,
                        kernel_size=3,stride=stride,padding=1,bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        # 用于恢复通道数
        self.conv3 = QuantizedConv2d(planes,self.expansion * planes,kernel_size=1,bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion * planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                QuantizedConv2d(in_planes,self.expansion * planes,
                          kernel_size=1,stride=stride,bias=False),
                nn.BatchNorm2d(self.expansion * planes)
            )

    def forward(self,x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    # block表示残差块的类型 可以是BasicBlock 或 Bottleneck
    # num_blocks: 是一个列表，列表中的每个元素表示每个残差层中的残差块的数量
    def __init__(self,block,num_blocks,num_classes=10,dataset='cifar10'):
        super(ResNet,self).__init__()
        self.in_planes = 64

        # different expansions for the blocks
        if 'cifar10' == dataset:
            self.fexpansion = block.expansion
        elif 'tiny-imagenet' == dataset:
            self.fexpansion = 4
        else:
            assert False,('Error: undefined for AlexNet - {}'.format(dataset))

        # features
        self.conv1 = QuantizedConv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        # 构建四个残差层，每个残差层包含多个残差块，随着层的加深，输出通道数逐渐增加
        self.layer1 = self._make_layer(block,64,num_blocks[0],stride=1)
        self.layer2 = self._make_layer(block,128,num_blocks[1],stride=2)
        self.layer3 = self._make_layer(block,256,num_blocks[2],stride=2)
        self.layer4 = self._make_layer(block,512,num_blocks[3],stride=2)
        # self.linear = QuantizedLinear(512 * block.expansion,num_classes)  old one
        self.linear = QuantizedLinear(512 * self.fexpansion,num_classes)


    def _make_layer(self,block,planes,num_blocks,stride):
        strides = [stride] + [1] * (num_blocks-1) # 第一个残差块的步长为stride，其余为1
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes,planes,stride))
            self.in_planes = planes * block.expansion # 更新输入通道数为当前输出的通道数
        return nn.Sequential(*layers)


    def forward(self,x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out,4)
        out = out.view(out.size(0),-1)
        out = self.linear(out)
        return out




def ResNet18(num_classes=10, dataset='cifar10'):
    return ResNet(BasicBlock, [2, 2, 2, 2], \
            num_classes=num_classes, dataset=dataset)


def ResNet34(num_classes=10, dataset='cifar10'):
    return ResNet(BasicBlock, [3, 4, 6, 3], \
            num_classes=num_classes, dataset=dataset)


def ResNet50(num_classes=10, dataset='cifar10'):
    return ResNet(Bottleneck, [3, 4, 6, 3], \
            num_classes=num_classes, dataset=dataset)


def ResNet101(num_classes=10, dataset='cifar10'):
    return ResNet(Bottleneck, [3, 4, 23, 3], \
            num_classes=num_classes, dataset=dataset)


def ResNet152(num_classes=10, dataset='cifar10'):
    return ResNet(Bottleneck, [3, 8, 36, 3], \
            num_classes=num_classes, dataset=dataset)