# 先学习https://blog.csdn.net/qq_54499870/article/details/127011866

"""
    VGG (w. Quantized Operations)
"""
# torch
import math
import torch
import torch.nn as nn

# custom
from utils.qutils import QuantizedConv2d, QuantizedLinear


# ---------------------------------------------------------
#   Globals
# ---------------------------------------------------------
__all__ = [
    'VGG',
    'VGG11', 'VGG11_BN', 'VGG13', 'VGG13_BN',
    'VGG16', 'VGG16_BN', 'VGG19', 'VGG19_BN'
]

# ---------------------------------------------------------------------
#   VGGs
# ---------------------------------------------------------------------
class VGG(nn.Module):
    """
        VGG model 用于图像分类任务
    """
    # features是一个nn.Sequential对象，包含了卷积层和池化层等特征提取部分的网络架构
    # num_classes 是分类的类别数，默认是10
    # dataset 数据集的名称，默认为‘cifar10’
    def __init__(self,features,num_classes=10,dataset='cifar10'):
        super(VGG,self).__init__()

        # different features for different dataset
        # 根据不同的数据集设置特征维度self.fdims
        if 'cifar10' == dataset:
            self.fdims = [512,1,1]
        elif 'tiny-imagenet' == dataset:
            self.fdims = [512,2,2]
        else:
            assert False, ('Error: undefined for AlexNet - {}'.format(dataset))

        # set the layers 设置模型的层结构
        self.features = features
        # 定义了一个全连接层的分类器，使用nn.Sequential按顺序堆叠多个层
        self.clasifier = nn.Sequential(
            nn.Dropout(),  # 随机丢弃部分神经元，防止过拟合
            QuantizedLinear(self.fdims[0] * self.fdims[1] * self.fdims[2],512), # 自定义的量化线性层，进行线性变换
            nn.ReLU(True), # 使用激活函数引入非线性
            nn.Dropout(),
            QuantizedLinear(512,512),
            nn.ReLU(True),
            QuantizedLinear(512,num_classes) # 将输出维度映射到分类的类别数
        )

        # Initialize weights 初始化模型的参数
        # 遍历模型中的所有模块
        for m in self.modules():
            if isinstance(m,nn.Conv2d):
                # 对于 nn.Conv2d类型的模块，使用Xavier初始化方法初始化卷积层的权重
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels # 计算卷积核的参数数量（卷积核的高度乘以宽度乘以输出通道数）
                m.weight.data.normal_(0,math.sqrt(2. / n)) # 使用正态分布初始化卷积层的权重
                m.bias.data.zero_() # 将卷积层的偏置项初始化为零

    def forward(self,x):
        x = self.features(x)
        x = x.view(x.size(0),-1)
        x = self.clasifier(x)
        return x


# 根据给定的配置列表cfg构建卷积神经网络中的卷积层和池化层部分
# 最终返回一个nn.Sequential对象，该对象按顺序包含了所构建的各个层
def make_layers(cfg,batch_norm=False):      # 相当于构建features特征提取架构
    layers = []
    in_channels = 3
    for v in cfg:
        if v == 'M':
            layers += [nn.MaxPool2d(kernel_size=2,stride=2)]  # 表示要构建一个最大池化层
        else:
            conv2d = QuantizedConv2d(in_channels,v,kernel_size=3,padding=1) # 此时v是一个整数，表示卷积层的输出通道数
            if batch_norm:  # 是否添加批归一化层
                layers += [conv2d,nn.BatchNorm2d(v),nn.ReLU(inplace=True)]
            else:
                layers += [conv2d,nn.ReLU(inplace=True)]

            in_channels = v

    return nn.Sequential(*layers)

cfg = {
    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',
          512, 512, 512, 512, 'M'],
}


def VGG11(num_classes=10, dataset='cifar10'):
    """VGG 11-layer model (configuration "A")"""
    return VGG(make_layers(cfg['A']), num_classes=num_classes, dataset=dataset)


def VGG13(num_classes=10, dataset='cifar10'):
    """VGG 13-layer model (configuration "B")"""
    return VGG(make_layers(cfg['B']), num_classes=num_classes, dataset=dataset)


def VGG16(num_classes=10, dataset='cifar10'):
    """VGG 16-layer model (configuration "D")"""
    return VGG(make_layers(cfg['D']), num_classes=num_classes, dataset=dataset)


def VGG19(num_classes=10, dataset='cifar10'):
    """VGG 19-layer model (configuration "E")"""
    return VGG(make_layers(cfg['E']), num_classes=num_classes, dataset=dataset)


def VGG11_BN(num_classes=10, dataset='cifar10'):
    """VGG 11-layer model (configuration "A") with batch normalization"""
    return VGG(make_layers(cfg['A'], batch_norm=True), num_classes=num_classes, dataset=dataset)


def VGG13_BN(num_classes=10, dataset='cifar10'):
    """VGG 13-layer model (configuration "B") with batch normalization"""
    return VGG(make_layers(cfg['B'], batch_norm=True), num_classes=num_classes, dataset=dataset)


def VGG16_BN(num_classes=10, dataset='cifar10'):
    """VGG 16-layer model (configuration "D") with batch normalization"""
    return VGG(make_layers(cfg['D'], batch_norm=True), num_classes=num_classes, dataset=dataset)


def VGG19_BN(num_classes=10, dataset='cifar10'):
    """VGG 19-layer model (configuration 'E') with batch normalization"""
    return VGG(make_layers(cfg['E'], batch_norm=True), num_classes=num_classes, dataset=dataset)

# 这是一个非常经典的VGG模型，里面会有几个报错的地方，是自定义的卷积层和自定义的线性层，先不用管，注意看一下怎么提取特征（提取特征的模块），提取完特征对特真正进行线性变换，然后输入到分类器里面，做最终的预测，最核心的是forward的函数，看一下怎么构建相关模块实现的
